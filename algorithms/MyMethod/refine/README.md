# idea to do：
    * 首先可以考虑GCN传播的选择性，就是传播那些在每一层都对齐的结点给他一个更大的传播系数？
    * 其次考虑GCN训练的时候的邻域一致性loss，要取到最大的MNC，加一个-MNC的loss？

# 模型表现情况(GCN最后一层embedding(edge noise + attribute noise) + MNC-refine):
    1. Arenas数据集：(人造)
        * 0.05 noise: 99%
        * 0.25 noise: 97.26% REAGL + refine:0.057
    2. 真实的数据集(不太稳定)
        1. douban-online vs offline: 0.07 
            这个数据集目前分析，可能存在的问题在于：
            * MNC并不能在这个数据集上得到很好的保证，GALIGN对齐效果不错完全是因为其属性的作用
            * 边的信息比较少，源网络8164，目标网络1151，但是目标网络的节点数却有1118，说明目标网络并不是全连通的，所以肯定用MNC的方法不太恰当在这里
            发现迭代过程之中，其MNC在上升，随后下降。
            最大值是0.32的准确率，这个值小于Galign的0.49，但是它考虑了三层的输出，并且因为第一层的输出跟feature严重相关，所以对齐准确率高？
    3. allmv-imdb: 0.90510
            这个数据集的MNC保持的很好，边的信息丰富一点
